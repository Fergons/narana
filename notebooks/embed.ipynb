{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efca9488380644c5acaa9c1230038228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\narana\\.venv\\Lib\\site-packages\\FlagEmbedding\\BGE_M3\\modeling.py:335: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  colbert_state_dict = torch.load(os.path.join(model_dir, 'colbert_linear.pt'), map_location='cpu')\n",
      "d:\\narana\\.venv\\Lib\\site-packages\\FlagEmbedding\\BGE_M3\\modeling.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sparse_state_dict = torch.load(os.path.join(model_dir, 'sparse_linear.pt'), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 East Palace has 932927 characters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (226411 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 East Palace has 226411 tokens.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from pathlib import Path\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "book = '109 East Palace'\n",
    "book_file_path = Path('..') / 'data' / 'bookcompanion'  / f'{book}.txt'\n",
    "book_content = book_file_path.read_text(encoding='utf-8')\n",
    "print(f\"{book} has {len(book_content)} characters.\")\n",
    "book_tokenized = model.tokenizer(book_content, return_tensors='pt')['input_ids']\n",
    "print(f\"{book} has {len(book_tokenized[0])} tokens.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length=8192\n",
      "torch.Size([1, 226411])\n"
     ]
    }
   ],
   "source": [
    "max_length = model.tokenizer.model_max_length\n",
    "print(f'{max_length=}')\n",
    "print(book_tokenized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_chunks = torch.chunk(book_tokenized,chunks=(len(book_tokenized[0])//max_length)+1,dim=1)\n",
    "# print(f'{len(book_tokenized[0])=}')\n",
    "# print(f'{len(naive_chunks)=}')\n",
    "# print(naive_chunks[0].shape)\n",
    "# # pad each tensor in naive_chunks to max_length\n",
    "# naive_chunks = [torch.nn.functional.pad(chunk, (0, max_length - chunk.shape[1])) for chunk in naive_chunks]\n",
    "# batch_of_chunks = torch.stack(naive_chunks).squeeze(1)\n",
    "# print(batch_of_chunks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks=(book_tokenized.shape[1]//max_length)+1\n",
    "chunk_len = len(book_content)//num_chunks\n",
    "batch_of_text_chunks = [book_content[i:i+chunk_len] for i in range(0, len(book_content), chunk_len)]\n",
    "vectors = model.encode(batch_of_text_chunks, max_length=max_length,batch_size=4,return_colbert_vecs=True, return_dense=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(batch_of_text_chunks)=29\n",
      "len(vectors)=3\n",
      "dict_keys(['dense_vecs', 'lexical_weights', 'colbert_vecs'])\n",
      "(8035, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(batch_of_text_chunks)=}')\n",
    "print(f'{len(vectors)=}')\n",
    "print(vectors.keys())\n",
    "print(vectors['colbert_vecs'][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(chunked_vecs)=40\n",
      "chunked_vecs[0].shape=torch.Size([201, 1024])\n",
      "book_tokenized={'input_ids': tensor([[    0,  9804, 13055,  ...,     1,     1,     1],\n",
      "        [    0,    28,  1295,  ...,     1,     1,     1],\n",
      "        [    0,   104,     4,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,    91,  2837,  ...,  4516,     4,     2],\n",
      "        [    0, 26759,    18,  ...,  2480,    16,     2],\n",
      "        [    0,   111, 15672,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "sum(col_vecs_size)=225482\n",
      "sum(book_tokenized_size)=225482\n",
      "tensor([[    0,  9804, 13055,  ...,     1,     1,     1],\n",
      "        [    0,    28,  1295,  ...,     1,     1,     1],\n",
      "        [    0,   104,     4,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,    91,  2837,  ...,  4516,     4,     2],\n",
      "        [    0, 26759,    18,  ...,  2480,    16,     2],\n",
      "        [    0,   111, 15672,  ...,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "def chunk_colbert_vecs(vecs):\n",
    "    num_tokens = len(vecs)\n",
    "    if num_tokens//100 > 0:\n",
    "        chunked_vecs = torch.chunk(torch.tensor(vecs), chunks=num_tokens//200)\n",
    "    else:\n",
    "        chunked_vecs = (vecs,)\n",
    "    \n",
    "    return chunked_vecs\n",
    "\n",
    "chunked_vecs = chunk_colbert_vecs(vectors['colbert_vecs'][0])\n",
    "print(f'{len(chunked_vecs)=}')\n",
    "print(f'{chunked_vecs[0].shape=}')\n",
    "chunk_len = chunked_vecs[0].shape[0]\n",
    "book_tokenized = model.tokenizer(\n",
    "                batch_of_text_chunks,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                max_length=max_length,\n",
    "            )\n",
    "print(f'{book_tokenized=}')\n",
    "col_vecs_size = [v.shape[0] for v in vectors['colbert_vecs']]\n",
    "book_chunks = []\n",
    "for tokens, attn_mask in zip(book_tokenized['input_ids'], book_tokenized['attention_mask']):\n",
    "    token_num = torch.sum(attn_mask)\n",
    "    book_chunks.append(tokens[:token_num-1])\n",
    "\n",
    "book_tokenized_size = [b.shape[0] for b in book_chunks]\n",
    "print(f'{sum(col_vecs_size)=}')\n",
    "print(f'{sum(book_tokenized_size)=}')\n",
    "print(f'{book_tokenized['input_ids']}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_chunks)=1114\n",
      "all_chunks[0].shape=torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frantisek\\AppData\\Local\\Temp\\ipykernel_27412\\2418520908.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_chunks = [torch.tensor(chunk).mean(dim=0) for doc in all_chunks for chunk in doc]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1024])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks = [chunk_colbert_vecs(v) for v in vectors['colbert_vecs']]\n",
    "all_chunks = [torch.tensor(chunk).mean(dim=0) for doc in all_chunks for chunk in doc]\n",
    "\n",
    "\n",
    "print(f'{len(all_chunks)=}')\n",
    "print(f'{all_chunks[0].shape=}')\n",
    "sims = torch.asarray([1,2,3])\n",
    "db = torch.stack(all_chunks)\n",
    "\n",
    "query = \"What is trinity site?\"\n",
    "vec= model.encode(query, return_colbert_vecs=True, return_dense=True)\n",
    "vec = torch.tensor(vec['colbert_vecs'])\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_max=torch.return_types.topk(\n",
      "values=tensor([0.2994, 0.2614, 0.2515]),\n",
      "indices=tensor([   0, 1073, 1113]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2994, 0.2006, 0.1894,  ..., 0.2055, 0.1914, 0.2515])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_ready = vec.mean(dim=0)\n",
    "# cosine similarity\n",
    "sim_scores = torch.cosine_similarity(db, q_ready, dim=1)\n",
    "sim_max = torch.topk(sim_scores,3)\n",
    "sim_index = torch.topk(sim_scores,3).indices\n",
    "print(f'{sim_max=}')\n",
    "sim_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8035\n"
     ]
    }
   ],
   "source": [
    "print(len(book_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_chunks_new = torch.concat(book_chunks)\n",
    "book_chunks_new = torch.chunk(book_chunks_new, chunks=len(all_chunks))\n",
    "len(book_chunks_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ALSO BY JENNET CONANT Tuxedo Park:A Wall Street Tycoon and the Secret Palace of Science That Changed the Course of World War II SIMON & SCHUSTER Rockefeller Center 1230 Avenue of the Americas New York, NY 10020 Copyright © 2005 by Jennet Conant All rights reserved, including the right of reproduction in whole or in part in any form. SIMON & SCHUSTER and colophon are registered trademarks of Simon & Schuster, Inc. Library of Congress Cataloging-in-Publication Data Conant, Jennet. 109 East Palace : Robert Oppenheimer and the secret city of Los Alamos / Jennet Conant. p. cm. Includes bibliographical references. 1. Los Alamos Scientific Laboratory—History. 2. Manhattan Project (U.S.)—History. 3. Atomic bomb—United States—History. 4. McKibbin, Dorothy\n",
      "New York. QUOTATIONS FROM NEWSPAPER ARTICLES AND MAGAZINES “Baggage, Babies and the Atom Bomb: The Unique 20 years of Dorothy McKibbin.” Los Alamos Scientific Laboratory News, June 28, 1963. Corbett, Peggy. “AEC Office in SF Closes.” The New Mexican, July 30,1963. ———. “Oppie’s Vitality Swayed Santa Fean.” The New Mexican, June 26, 1960. Hall, Rosanna. “Dorothy McKibbin Remembers Early Days in NM.” The New Mexican, June 11, 1981. McMaho, June. “‘109’ to Close: Dorothy McKibbin Retirement Told.” The Los Alamos Monitor, June 27, 1963. McNulty, William. “World’s Most Famed Scientists, En Route to Los Alamos Project, Go Through Ancient\n"
     ]
    }
   ],
   "source": [
    "for idx in sim_index:\n",
    "    if idx >= len(book_chunks_new):\n",
    "        continue\n",
    "    print(model.tokenizer.decode(book_chunks_new[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 364/364 [01:19<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "naive_chunks = nltk.sent_tokenize(book_content)\n",
    "vectors= model.encode(naive_chunks,batch_size=16, return_dense=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.6450, 0.6426, 0.6313], dtype=torch.float16),\n",
       "indices=tensor([3742, 2288, 1312]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Relationship between albert einstein and oppenheimer's girlfriend?\"\n",
    "qv = model.encode(query, return_dense=True)\n",
    "sims = torch.cosine_similarity(torch.tensor(vectors['dense_vecs']), torch.tensor(qv['dense_vecs']), dim=1)\n",
    "torch.topk(sims,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Oppenheimer made her one of his famous martinis and introduced her around.\n",
      "------\n",
      "Oppenheimer ended up missing his train and stayed the night in her apartment.\n",
      "------\n",
      "Oppenheimer asked her to come with him and offered her a position on his personal staff.\n",
      "------\n",
      "Oppenheimer kept putting her off.\n",
      "------\n",
      "“The trouble with Oppenheimer is that he loves a woman who doesn’t love him—the United States Government,” Einstein observed of his Princeton colleague, with whom he was never particularly close.Oppenheimer had been humbled, but not destroyed.\n"
     ]
    }
   ],
   "source": [
    "for idx in torch.topk(sims,5).indices:\n",
    "    if idx >= len(naive_chunks):\n",
    "        continue\n",
    "    print('------')\n",
    "    print(naive_chunks[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
