{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 East Palace has 932927 characters.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('BAAI/bge-m3', trust_remote_code=True)\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "model.half()\n",
    "\n",
    "book = '109 East Palace'\n",
    "book_file_path = Path('..') / 'data' / 'bookcompanion'  / f'{book}.txt'\n",
    "book_content = book_file_path.read_text(encoding='utf-8')\n",
    "print(f\"{book} has {len(book_content)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(len(tokenizer(test_text, return_tensors='pt')['input_ids'][0]))\n",
    "print(len(tokenizer(test_text, return_tensors='pt', add_special_tokens=False)['input_ids'][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "           \n",
    "from nltk import sent_tokenize\n",
    "\n",
    "def sentence_chunking(input_text: str, tokenizer: callable):\n",
    "    sentences = sent_tokenize(input_text)\n",
    "    last_offset_index = 0\n",
    "    final_chunk = []\n",
    "    spans = []\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence, return_tensors='pt', add_special_tokens=False)\n",
    "        input_ids = tokens['input_ids'][0]\n",
    "        final_chunk.extend(input_ids)\n",
    "        spans.append((last_offset_index, last_offset_index + len(input_ids)))\n",
    "        last_offset_index += len(input_ids)\n",
    "    return sentences, final_chunk, spans\n",
    "\n",
    "\n",
    "\n",
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_text = book_content\n",
    "# determine chunks\n",
    "chunks, tokenized, spans = sentence_chunking(input_text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "As the gatekeeper to Los Alamos, she presented herself as a peculiarly compelling witness to history, registering the full scope of the momentous change and moral upheaval the scientists’ work unleashed.\n",
      "As the gatekeeper to Los Alamos, she presented herself as a peculiarly compelling witness to history, registering the full scope of the momentous change and moral upheaval the scientists’ work unleashed.\n",
      "---\n",
      "She was not objective in any real sense, but for that matter, neither am I.\n",
      "She was not objective in any real sense, but for that matter, neither am I.\n",
      "---\n",
      "She was smitten with Robert Oppenheimer from the moment they met and unreservedly embraced both him and his brilliant crew of scientists, including my grandfather, whom she liked and admired.\n",
      "She was smitten with Robert Oppenheimer from the moment they met and unreservedly embraced both him and his brilliant crew of scientists, including my grandfather, whom she liked and admired.\n",
      "---\n",
      "But as an intelligent, articulate, and knowing observer, she made the human element all the more vivid and understandable, offering a unique view of the atomic pioneers that led the way to the Trinity test and a dangerous new world.\n",
      "But as an intelligent, articulate, and knowing observer, she made the human element all the more vivid and understandable, offering a unique view of the atomic pioneers that led the way to the Trinity test and a dangerous new world.\n",
      "---\n",
      "Dorothy chronicled the dramatic times, problems, and mounting conflicts in her unpublished memoir and many letters to Oppenheimer.\n",
      "Dorothy chronicled the dramatic times, problems, and mounting conflicts in her unpublished memoir and many letters to Oppenheimer.\n",
      "---\n",
      "She made no claim to heroism for her own small role in history, but her record of courage, service, and sacrifice, along with that of Oppenheimer and the hundreds of remarkable men and women who followed him to that high mesa in New Mexico, may give us a new understanding of what they achieved in those unparalleled twenty-seven months.\n",
      "She made no claim to heroism for her own small role in history, but her record of courage, service, and sacrifice, along with that of Oppenheimer and the hundreds of remarkable men and women who followed him to that high mesa in New Mexico, may give us a new understanding of what they achieved in those unparalleled twenty-seven months.\n",
      "---\n",
      "And with America at war again over weapons of mass destruction, her story also gives us a new appreciation for the need to promote peace and defuse the nuclear predicament they unwittingly helped to create.\n",
      "And with America at war again over weapons of mass destruction, her story also gives us a new appreciation for the need to promote peace and defuse the nuclear predicament they unwittingly helped to create.\n"
     ]
    }
   ],
   "source": [
    "for chunk, span in zip(chunks[68:75], spans[68:75]):\n",
    "   print('---')\n",
    "   print(chunk)\n",
    "   x,y = span\n",
    "   print(tokenizer.decode(tokenized[x:y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk before\n",
    "batch_size = 8\n",
    "embeddings_traditional_chunking = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, 16, batch_size):\n",
    "        tokenized_chunks = tokenizer(chunks[i:i+batch_size], return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
    "        outputs=model(**tokenized_chunks)[0].detach().cpu().numpy()\n",
    "        embeddings_traditional_chunking.extend(outputs)\n",
    "        print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(chunks[i:i\u001b[38;5;241m+\u001b[39mbatch_size], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m model_output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m----> 8\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mextend(\u001b[43mlate_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspan_annotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m, in \u001b[0;36mlate_chunking\u001b[1;34m(model_output, span_annotation, max_length)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embeddings, annotations \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(token_embeddings, span_annotation):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m         max_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     ):  \u001b[38;5;66;03m# remove annotations which go bejond the max-length of the model\u001b[39;00m\n\u001b[0;32m     41\u001b[0m         annotations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     42\u001b[0m             (start, \u001b[38;5;28mmin\u001b[39m(end, max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 43\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m (start, end) \u001b[38;5;129;01min\u001b[39;00m annotations\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m<\u001b[39m (max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     45\u001b[0m         ]\n\u001b[0;32m     46\u001b[0m     pooled_embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     47\u001b[0m         embeddings[start:end]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m annotations\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (end \u001b[38;5;241m-\u001b[39m start) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     50\u001b[0m     ]\n\u001b[0;32m     51\u001b[0m     pooled_embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     52\u001b[0m         embedding\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m pooled_embeddings\n\u001b[0;32m     53\u001b[0m     ]\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "# chunk afterwards (context-sensitive chunked pooling)\n",
    "max_length = 8192\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        inputs = tokenizer(chunks[i:i+batch_size], return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
    "        model_output = model(**inputs)\n",
    "        embeddings.extend(late_chunking(model_output, span_annotations[i:i+batch_size], max_length=max_length)[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.07544, -0.0696 , -0.6836 , ..., -0.311  , -0.702  , -0.3406 ],\n",
       "       dtype=float16),\n",
       " array([-0.05313, -0.0354 , -0.00892, ...,  0.00399, -0.08466,  0.02725],\n",
       "       dtype=float16),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float16),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float16),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float16),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float16),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float16),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float16),\n",
       " array([-0.0882 ,  0.00818, -0.5522 , ...,  0.7456 , -0.6885 ,  0.3816 ],\n",
       "       dtype=float16),\n",
       " array([-0.0882 ,  0.00818, -0.5522 , ...,  0.7456 , -0.689  ,  0.3816 ],\n",
       "       dtype=float16),\n",
       " array([-0.0882 ,  0.00818, -0.5522 , ...,  0.7456 , -0.6885 ,  0.3816 ],\n",
       "       dtype=float16)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XLMRobertaModel' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m cos_sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, y: np\u001b[38;5;241m.\u001b[39mdot(x, y) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(y))\n\u001b[1;32m----> 5\u001b[0m berlin_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain protagonist character in the narrative\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, new_embedding, trad_embeddings \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunks, embeddings, embeddings_traditional_chunking):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_new(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBerlin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m'\u001b[39m, cos_sim(berlin_embedding, new_embedding))\n",
      "File \u001b[1;32md:\\narana\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'XLMRobertaModel' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "q = 'Main protagonist character in the narrative'\n",
    "tokenized_q = tokenizer(q, return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
    "q_embedding = model(**tokenized_q)[0].detach().cpu().numpy()\n",
    "\n",
    "# for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "#     print(f'similarity_new(\"{q}\", \"{chunk}\"):', cos_sim(q_embedding, new_embedding))\n",
    "#     print(f'similarity_trad(\"{q}\", \"{chunk}\"):', cos_sim(q_embedding, trad_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "for c in embeddings:\n",
    "    print(c.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
